{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from commonfunctions import *\n",
    "from Pre_Processing_Functions import *\n",
    "from skimage.transform import resize\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import cv2\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Supporting functions\n",
    "\n",
    "def HoG(image):\n",
    "    # Resize the image to 130x276\n",
    "    resized_img = cv2.resize(image, (64, 128))\n",
    "    # print(resized_img.shape)\n",
    "\n",
    "    # Convert the original image to gray scale\n",
    "    resized_img_gray = cv2.cvtColor(resized_img, cv2.COLOR_BGR2GRAY)\n",
    "    '''\n",
    "    # Specify the parameters for our HOG descriptor\n",
    "    win_size = (64, 128)  # You need to set a proper window size\n",
    "    block_size = (16, 16)\n",
    "    block_stride = (8, 8)\n",
    "    cell_size = (8, 8)\n",
    "    num_bins = 9\n",
    "    '''\n",
    "    \n",
    "    win_size = (64, 128)  # Increase the window size\n",
    "    block_size = (32, 32)  # Increase the block size\n",
    "    block_stride = (16, 16)  # Increase the block stride\n",
    "    cell_size = (16, 16)  # Increase the cell size\n",
    "    num_bins = 18  # Increase the number of bins\n",
    "\n",
    "    # Set the parameters of the HOG descriptor using the variables defined above\n",
    "    hog = cv2.HOGDescriptor(win_size, block_size, block_stride, cell_size, num_bins)\n",
    "\n",
    "    # Compute the HOG Descriptor for the gray scale image\n",
    "    hog_descriptor = hog.compute(resized_img_gray)\n",
    "    return hog_descriptor\n",
    "\n",
    "\n",
    "def img_pre_processing(image):\n",
    "\n",
    "    # (1) crop background\n",
    "    #cropped_image=crop_background(image)\n",
    "\n",
    "    # (2) Apply Gaussian filtering to smooth out the image and remove any noise that may affect the accuracy of the HOG feature extractor\n",
    "    #we need to try different kernel sizes for the best!\n",
    "    gaussian_image = cv2.GaussianBlur(image, (3, 3), 0)\n",
    "\n",
    "    # Convert the image to floating point\n",
    "    gaussian_image_float = gaussian_image.astype(np.float32)\n",
    "\n",
    "    # Calculate the histogram\n",
    "    hist = cv2.calcHist([image], [0], None, [256], [0,256])\n",
    "\n",
    "    # Calculate the mean of the histogram\n",
    "    mean_hist = np.mean(hist)\n",
    "\n",
    "    # Convert the image to HSV color space\n",
    "    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Extract the S channel\n",
    "    s_channel = hsv_image[:, :, 1]\n",
    "\n",
    "    # Calculate the average saturation\n",
    "    average_saturation = np.mean(s_channel)\n",
    "\n",
    "    # Decide the gamma value\n",
    "    if (average_saturation > 128) and (mean_hist > 128):\n",
    "        gamma = 1.2  # or any value greater than 1\n",
    "    else:\n",
    "        gamma = 0.8  # or any value less than 1\n",
    "\n",
    "    # Apply gamma correction\n",
    "    gamma_corrected_image = cv2.pow(gaussian_image_float/ 255, gamma)\n",
    "\n",
    "\n",
    "    final_image_before_HOG=gamma_corrected_image*255\n",
    "    \n",
    "    image=final_image_before_HOG.astype(np.uint8)\n",
    "    \n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image editing => Cropping,rotation\n",
    "def crop_image(image, box):\n",
    "    x, y, w, h = cv2.boundingRect(box)\n",
    "    if w < h:\n",
    "        w, h = h, w\n",
    "    return image[y:y + h, x:x + w]\n",
    "\n",
    "def extract_boundaries(image):\n",
    "    # Read the image\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply GaussianBlur to reduce noise and improve edge detection\n",
    "    blurred = cv2.GaussianBlur(image, (3, 3), 0)\n",
    "\n",
    "    # Use the Canny edge detector\n",
    "    edges = cv2.Canny(blurred, 50, 150)\n",
    "\n",
    "    return edges\n",
    "\n",
    "def fill_small_holes(image, kernel_size=3):\n",
    "    # Read the image\n",
    "    image = extract_boundaries(image)\n",
    "\n",
    "    # Apply GaussianBlur to reduce noise\n",
    "    blurred = cv2.GaussianBlur(image, (3, 3), 0)\n",
    "\n",
    "    # Apply Canny edge detection\n",
    "    edges = cv2.Canny(blurred, 50, 150)\n",
    "\n",
    "    # Define a kernel for morphological operations\n",
    "    kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
    "\n",
    "    # Perform closing to fill small holes\n",
    "    closed = cv2.morphologyEx(edges, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "    # Perform erosion\n",
    "    dilated_edges = cv2.dilate(edges, kernel, iterations=1)\n",
    "    eroded_edges = cv2.erode(dilated_edges, kernel, iterations=1)\n",
    "\n",
    "    return eroded_edges\n",
    "\n",
    "def auto_rotate_image(image, counter=1):\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply GaussianBlur to reduce noise and aid contour detection\n",
    "    blurred = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "\n",
    "    # Use Canny edge detection\n",
    "    edges = cv2.Canny(blurred, 50, 150)\n",
    "\n",
    "    # Find contours in the edged image\n",
    "    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    image_with_contour = image.copy()\n",
    "\n",
    "    # Find the contour with the maximum area\n",
    "    max_contour = max(contours, key=cv2.contourArea)\n",
    "\n",
    "    # Fit a rotated bounding box to the contour\n",
    "    rect = cv2.minAreaRect(max_contour)\n",
    "\n",
    "    # Unpack the rectangle information\n",
    "    (center, (width_contour, height_contour), angle) = rect\n",
    "\n",
    "    # Ensure width is always the larger side\n",
    "    if width_contour < height_contour:\n",
    "        width_contour, height_contour = height_contour, width_contour\n",
    "        angle -= 90\n",
    "\n",
    "    height, width = image.shape[:2]\n",
    "    width_ratio = float(width_contour / width)\n",
    "\n",
    "    #Counter is for the second call of this function, in this call I am interested in getting the new coordinates of the bounding box but no further rotation needed.\n",
    "    if counter == 0 and width_ratio > 0.65:\n",
    "\n",
    "        # Rotate the image to the detected angle\n",
    "        rotated_image = rotating_image(image_with_contour, angle)\n",
    "    else:\n",
    "        return image_with_contour, max_contour\n",
    "\n",
    "    return rotated_image, max_contour\n",
    "\n",
    "def rotating_image(image, angle):\n",
    "    # Get the image dimensions\n",
    "    height, width = image.shape[:2]\n",
    "\n",
    "    # Calculate the rotation matrix\n",
    "    rotation_matrix = cv2.getRotationMatrix2D((width / 2, height / 2), angle, 1)\n",
    "\n",
    "    # Determine the new bounding box after rotation\n",
    "    cos_theta = np.abs(rotation_matrix[0, 0])\n",
    "    sin_theta = np.abs(rotation_matrix[0, 1])\n",
    "\n",
    "    new_width = int((width * cos_theta) + (height * sin_theta))\n",
    "    new_height = int((width * sin_theta) + (height * cos_theta))\n",
    "\n",
    "    # Adjust the rotation matrix for translation to keep the entire rotated image\n",
    "    rotation_matrix[0, 2] += (new_width - width) / 2\n",
    "    rotation_matrix[1, 2] += (new_height - height) / 2\n",
    "\n",
    "    # Apply the rotation to the image\n",
    "    rotated_image = cv2.warpAffine(image, rotation_matrix, (new_width, new_height),\n",
    "                                    borderMode=cv2.BORDER_CONSTANT, borderValue=(255, 255, 255))\n",
    "\n",
    "    return rotated_image\n",
    "\n",
    "#Test function\n",
    "\n",
    "def Background_Removal():\n",
    "    images_folder = 'SAR_Background/'\n",
    "    images = os.listdir(images_folder)\n",
    "\n",
    "    for image in images:\n",
    "        img_path = os.path.join(images_folder, image)\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is not None:\n",
    "            new_image, contour1 = auto_rotate_image(img, 0)\n",
    "\n",
    "            rect1 = cv2.minAreaRect(contour1)\n",
    "            box1 = cv2.boxPoints(rect1)\n",
    "            box1 = np.int0(box1)\n",
    "\n",
    "            # Unpack the rectangle information\n",
    "            (center1, (width_contour1, height_contour1), angle1) = rect1\n",
    "\n",
    "            # Ensure width is always the larger side\n",
    "            if width_contour1 < height_contour1:\n",
    "                width_contour1, height_contour1 = height_contour1, width_contour1\n",
    "                angle1 -= 90\n",
    "\n",
    "            rotated_image, contour2 = auto_rotate_image(new_image)\n",
    "            rect2 = cv2.minAreaRect(contour2)\n",
    "            box2 = cv2.boxPoints(rect2)\n",
    "\n",
    "            (center2, (width_contour2, height_contour2), angle2) = rect2\n",
    "\n",
    "            # Ensure width is always the larger side\n",
    "            if width_contour2 < height_contour2:\n",
    "                width_contour2, height_contour2 = height_contour2, width_contour2\n",
    "                angle2 -= 90\n",
    "\n",
    "            if -1 < angle2 < 1 and height_contour2 > 50:\n",
    "                cropped_image = crop_image(rotated_image, box2)\n",
    "            elif -1 < angle1 < 1 and height_contour1 > 50:\n",
    "                new_height, new_width = rotated_image.shape[:2]\n",
    "                # Resize the image\n",
    "                resized_image = cv2.resize(new_image, (new_width, new_height))\n",
    "                cropped_image = crop_image(resized_image, box1)     \n",
    "            else:\n",
    "                cropped_image = rotated_image\n",
    "\n",
    "            show_images([cropped_image, rotated_image, new_image, img],['cropped','rotated','New_image','before'])\n",
    "\n",
    "        else:\n",
    "            print(f\"Cannot open {image}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HOG from scratch!\n",
    "from pathlib import Path\n",
    "#from typing import Final\n",
    "\n",
    "import numpy as np\n",
    "import skimage as sk\n",
    "\n",
    "\n",
    "def compute_gradient(img: np.ndarray, grad_filter: np.ndarray) -> np.ndarray:\n",
    "\n",
    "    ts = grad_filter.shape[0]\n",
    "\n",
    "    new_img = np.zeros((img.shape[0] + ts - 1, img.shape[1] + ts - 1))\n",
    "\n",
    "    new_img[int((ts-1)/2.0):img.shape[0] + int((ts-1)/2.0), \n",
    "            int((ts-1)/2.0):img.shape[1] + int((ts-1)/2.0)] = img\n",
    "\n",
    "    result = np.zeros((new_img.shape))\n",
    "    \n",
    "    for r in np.uint16(np.arange((ts-1)/2.0, img.shape[0]+(ts-1)/2.0)):\n",
    "        for c in np.uint16(np.arange((ts-1)/2.0, img.shape[1]+(ts-1)/2.0)):\n",
    "            curr_region = new_img[r-np.uint16((ts-1)/2.0):r+np.uint16((ts-1)/2.0)+1, \n",
    "                                  c-np.uint16((ts-1)/2.0):c+np.uint16((ts-1)/2.0)+1]\n",
    "            curr_result = curr_region * grad_filter\n",
    "            score = np.sum(curr_result)\n",
    "            result[r, c] = score\n",
    "\n",
    "    result_img = result[np.uint16((ts-1)/2.0):result.shape[0]-np.uint16((ts-1)/2.0), \n",
    "                        np.uint16((ts-1)/2.0):result.shape[1]-np.uint16((ts-1)/2.0)]\n",
    "\n",
    "    return result_img\n",
    "\n",
    "def compute_gradient_magnitude(horizontal_gradient: np.ndarray, vertical_gradient: np.ndarray) -> np.ndarray:\n",
    "\n",
    "    mag_img=np.sqrt(np.power(vertical_gradient,2)+np.power(horizontal_gradient,2))\n",
    "    return mag_img\n",
    "\n",
    "\n",
    "\n",
    "def compute_gradient_direction(horizontal_gradient: np.ndarray, vertical_gradient: np.ndarray) -> np.ndarray:\n",
    "\n",
    "    horizontal_gradient+=1e-5\n",
    "    img_dir=np.rad2deg(np.arctan(vertical_gradient/horizontal_gradient))%180\n",
    "    return img_dir\n",
    "\n",
    "\n",
    "\n",
    "def find_nearest_bins(curr_direction: float, hist_bins: np.ndarray) -> (int, int):\n",
    " \n",
    "    min=200\n",
    "    c=0\n",
    "    ind_first=10\n",
    "    ind_second=10\n",
    "    if (curr_direction>160):\n",
    "        diff=np.abs(curr_direction-hist_bins[8])\n",
    "        ind_first=8\n",
    "        ind_second=0\n",
    "    else:\n",
    "        for i in range(9):\n",
    "            new=np.floor(curr_direction // 10)\n",
    "            curr_direction=new * 10\n",
    "            diff=curr_direction-hist_bins[i]\n",
    "            if (diff<0):\n",
    "                diff=diff*-1\n",
    "            if (diff<min):\n",
    "                min=diff\n",
    "                c=i\n",
    "        ind_first=c\n",
    "        ind_second=c+1\n",
    "\n",
    "    return (ind_first,ind_second)\n",
    "\n",
    "\n",
    "def update_histogram_bins(\n",
    "        HOG_cell_hist: np.ndarray, \n",
    "        curr_direction: float, \n",
    "        curr_magnitude: float, \n",
    "        first_bin_idx: int, \n",
    "        second_bin_idx: int, \n",
    "        hist_bins: np.ndarray\n",
    "    ) -> None:\n",
    "\n",
    "    size=20\n",
    "    ratiosecond=(hist_bins[second_bin_idx]-curr_direction)/size\n",
    "    ratiofirst=1-ratiosecond\n",
    "    HOG_cell_hist[first_bin_idx]+=curr_magnitude*ratiofirst\n",
    "    HOG_cell_hist[second_bin_idx]+=curr_magnitude*ratiosecond\n",
    "\n",
    "\n",
    "def calculate_histogram_per_cell(\n",
    "        cell_direction: np.ndarray, \n",
    "        cell_magnitude: np.ndarray, \n",
    "        hist_bins: np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "\n",
    "\n",
    "    empty_array=np.zeros(hist_bins.shape)\n",
    "    \n",
    "    for i in range(8):\n",
    "        for j in range(8):\n",
    "            first,second=find_nearest_bins(cell_direction[i,j],hist_bins)\n",
    "            update_histogram_bins(empty_array,cell_direction[i,j],cell_magnitude[i,j],first,second,hist_bins)\n",
    "    return empty_array\n",
    "\n",
    "\n",
    "def compute_hog_features(image):\n",
    "\n",
    "    image = sk.transform.resize(image, (128, 64))\n",
    "\n",
    "    if image.shape[2] == 4:\n",
    "        image = image[:, :, :3]\n",
    "\n",
    "    if len(image.shape) == 3:\n",
    "        image = sk.color.rgb2gray(image)\n",
    "\n",
    "    # Define gradient masks\n",
    "    horizontal_mask = np.array([-1, 0, 1])\n",
    "    vertical_mask = np.array([[-1], [0], [1]])\n",
    "\n",
    "    # Compute gradients\n",
    "    horizontal_gradient = compute_gradient(image, horizontal_mask)\n",
    "    vertical_gradient = compute_gradient(image, vertical_mask)\n",
    "\n",
    "    # Compute gradient magnitude and direction\n",
    "    grad_magnitude = compute_gradient_magnitude(horizontal_gradient, vertical_gradient)\n",
    "    grad_direction = compute_gradient_direction(horizontal_gradient, vertical_gradient)\n",
    "\n",
    "    # Define histogram bins\n",
    "    hist_bins = np.array([10, 30, 50, 70, 90, 110, 130, 150, 170])\n",
    "\n",
    "    cells_histogram = np.zeros((16, 8, 9))\n",
    "    for r in range(0, grad_magnitude.shape[0], 8):\n",
    "        for c in range(0, grad_magnitude.shape[1], 8):\n",
    "            cell_direction = grad_direction[r:r+8, c:c+8]\n",
    "            cell_magnitude = grad_magnitude[r:r+8, c:c+8]\n",
    "            #calculate histogram per cell\n",
    "           # cells_histogram[r//8, c//8] = compute_cell_histogram(cell_direction, cell_magnitude, hist_bins)\n",
    "\n",
    "            cells_histogram[int(r/8), int(c/8)] = calculate_histogram_per_cell(cell_direction, cell_magnitude, hist_bins)\n",
    "\n",
    "    # Normalize and concatenate histograms\n",
    "    features_list = []\n",
    "    for r in range(cells_histogram.shape[0] - 1):\n",
    "        for c in range(cells_histogram.shape[1] - 1):\n",
    "            histogram_16x16 = np.reshape(cells_histogram[r:r+2, c:c+2], (36,))\n",
    "            histogram_16x16_normalized = histogram_16x16 / (np.linalg.norm(histogram_16x16) + 1e-5)\n",
    "            features_list.append(histogram_16x16_normalized)\n",
    "\n",
    "    return np.concatenate(features_list, axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM from scratch (53%)\n",
    "class Karim:\n",
    "\n",
    "    def __init__(self, C = 1.0):\n",
    "        # C = error term\n",
    "        self.C = C\n",
    "        self.w = 0\n",
    "        self.b = 0\n",
    "        \n",
    "       # Hinge Loss Function / Calculation\n",
    "    def hingeloss(self, w, b, x, y):\n",
    "        # Regularizer term\n",
    "        reg = 0.5 * (w * w)\n",
    "\n",
    "        for i in range(x.shape[0]):\n",
    "            # Optimization term\n",
    "            opt_term = y[i] * ((np.dot(w, x[i])) + b)\n",
    "\n",
    "        # calculating loss\n",
    "        loss = reg + self.C * max(0, 1 - opt_term)\n",
    "\n",
    "        return loss[0][0]\n",
    "    \n",
    "    def fit(self, X, Y, batch_size=100, learning_rate=0.001, epochs=1000):\n",
    "        # The number of features in X\n",
    "        number_of_features = X.shape[1]\n",
    "\n",
    "        # The number of Samples in X\n",
    "        number_of_samples = X.shape[0]\n",
    "\n",
    "        c = self.C\n",
    "\n",
    "        # Creating ids from 0 to number_of_samples - 1\n",
    "        ids = np.arange(number_of_samples)\n",
    "\n",
    "        # Shuffling the samples randomly\n",
    "        np.random.shuffle(ids)\n",
    "\n",
    "        # creating an array of zeros\n",
    "        w = np.zeros((1, number_of_features))\n",
    "        b = 0\n",
    "        losses = []\n",
    "\n",
    "        # Gradient Descent logic\n",
    "        for i in range(epochs):\n",
    "            # Calculating the Hinge Loss\n",
    "            l = self.hingeloss(w, b, X, Y)\n",
    "\n",
    "            # Appending all losses \n",
    "            losses.append(l)\n",
    "            \n",
    "            # Starting from 0 to the number of samples with batch_size as interval\n",
    "            for batch_initial in range(0, number_of_samples, batch_size):\n",
    "                gradw = 0\n",
    "                gradb = 0\n",
    "\n",
    "                for j in range(batch_initial, batch_initial + batch_size):\n",
    "                    if j < number_of_samples:\n",
    "                        x = ids[j]\n",
    "                        ti = Y[x] * (np.dot(w, X[x].T) + b)\n",
    "\n",
    "                        if ti > 1:\n",
    "                            gradw += 0\n",
    "                            gradb += 0\n",
    "                        else:\n",
    "                            # Calculating the gradients\n",
    "\n",
    "                            #w.r.t w \n",
    "                            gradw += c * Y[x] * X[x]\n",
    "                            # w.r.t b\n",
    "                            gradb += c * Y[x]\n",
    "\n",
    "                # Updating weights and bias\n",
    "                w = w + learning_rate * gradw\n",
    "                b = b + learning_rate * gradb\n",
    "        \n",
    "        self.w = w\n",
    "        self.b = b\n",
    "\n",
    "        return self.w, self.b, losses  \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \n",
    "        prediction = np.dot(X, self.w[0]) + self.b # w.x + b\n",
    "        return np.sign(prediction)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM from scratch (50%)\n",
    "class Karim2:\n",
    "\n",
    "    def __init__(self, C = 1.0):\n",
    "        # C = error term\n",
    "        self.C = C\n",
    "        self.w = 0\n",
    "        self.b = 0\n",
    "        \n",
    "    def compute_cost(self,x, y,w,b):\n",
    "   \n",
    "        m = x.shape[0] \n",
    "        cost = self.C\n",
    "        \n",
    "\n",
    "        for i in range(m):\n",
    "            f_wb = w * x[i] + b\n",
    "            cost = cost + (f_wb - y[i])**2\n",
    "        total_cost = 1 / (2 * m) * cost\n",
    "\n",
    "        return total_cost\n",
    "    \n",
    "    def compute_gradient(self,x, y,w,b): \n",
    "        # Number of training examples\n",
    "        m = x.shape[0] \n",
    "        cost=0\n",
    "        dj_dw = 0\n",
    "        dj_db = 0\n",
    "\n",
    "        for i in range(m):  \n",
    "            f_wb = w * x[i] + b \n",
    "            dj_dw_i = (f_wb - y[i]) * x[i] \n",
    "            dj_db_i = f_wb - y[i] \n",
    "            dj_db += dj_db_i\n",
    "            dj_dw += dj_dw_i \n",
    "        dj_dw = dj_dw / m \n",
    "        dj_db = dj_db / m \n",
    "\n",
    "        return dj_dw, dj_db\n",
    "    \n",
    "    def gradient_descent(self,x, y, alpha=0.0001, num_iters=10000): \n",
    "        \n",
    "        w=self.w\n",
    "        b=self.b\n",
    "\n",
    "        for i in range(num_iters):\n",
    "            # Calculate the gradient and update the parameters using gradient_function\n",
    "            dj_dw, dj_db = self.compute_gradient(x, y, w , b)   \n",
    "            \n",
    "            #calculate cost\n",
    "            if i<100000:      # prevent resource exhaustion \n",
    "                cost =  self.compute_cost(x, y, w, b)  \n",
    "\n",
    "            # Update Parameters using equation (3) above\n",
    "            b = b - alpha * dj_db                            \n",
    "            w = w - alpha * dj_dw\n",
    "            \n",
    "        self.w=w\n",
    "        self.b=b\n",
    "        self.C=cost\n",
    "    \n",
    "        return self.w, self.b,self.C\n",
    "    \n",
    "    def predict(self,x):\n",
    "        return np.sign(np.dot(x, self.w[0]) + self.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "input_dir = 'Full_Dataset/'\n",
    "#categories = ['1EGP','5EGP','10EGP','20EGP','50EGP','100EGP','200EGP']\n",
    "#categories = ['1EGP','5EGP','10EGP','20EGP','50EGP','100EGP','200EGP']\n",
    "#categories = ['1SAR','5SAR','10SAR','50SAR','100SAR','500SAR']\n",
    "categories = ['1EGP','1SAR', '5EGP','5SAR','10EGP','10SAR','20EGP','50EGP','50SAR','100EGP','100SAR','200EGP','500SAR']\n",
    "\n",
    "data = []\n",
    "feature_4 = []\n",
    "\n",
    "labels = []\n",
    "\n",
    "for category_idx, category in enumerate(categories):\n",
    "    for file in os.listdir(os.path.join(input_dir, category)):\n",
    "        img_path = os.path.join(input_dir, category, file)\n",
    "\n",
    "        img = io.imread(img_path)\n",
    "        processed_img=img_pre_processing(img)\n",
    "        #HOG_class=compute_hog_features(img)\n",
    "        image=processed_img.astype(np.uint8)\n",
    "        HOG_class=HoG(image)\n",
    "\n",
    "        feature_4.append(HOG_class.flatten())\n",
    "\n",
    "        labels.append(category_idx)\n",
    "\n",
    "\n",
    "data=np.asarray(feature_4)\n",
    "\n",
    "labels = np.asarray(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train / test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, shuffle=True, stratify=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=SVC(),\n",
       "             param_grid=[{'C': [1, 10, 100, 1000],\n",
       "                          'gamma': [0.01, 0.001, 0.0001]}])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train classifier\n",
    "classifier = SVC()\n",
    "#classifier.fit(x_train,y_train)\n",
    "\n",
    "parameters = [{'gamma': [0.01, 0.001, 0.0001], 'C': [1, 10, 100, 1000]}] #to find the best combination for better results\n",
    "\n",
    "grid_search = GridSearchCV(classifier, parameters)\n",
    "\n",
    "grid_search.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.24528301886792% of samples were correctly classified\n"
     ]
    }
   ],
   "source": [
    "# test performance\n",
    "best_estimator = grid_search.best_estimator_\n",
    "\n",
    "y_prediction = best_estimator.predict(x_test)\n",
    "\n",
    "score = accuracy_score(y_prediction, y_test)\n",
    "\n",
    "print('{}% of samples were correctly classified'.format(str(score * 100)))\n",
    "\n",
    "pickle.dump(best_estimator, open('./model_20fix.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing of SVM from scratch (50%)\n",
    "'''\n",
    "classifier = Karim2()\n",
    "classifier.gradient_descent(x_train,y_train)\n",
    "y_pred = classifier.predict(x_test)\n",
    "y_prediction=classifier.predict(x_test)\n",
    "y_prediction=y_prediction[:,0]\n",
    "print(y_prediction.shape)\n",
    "score=accuracy_score(y_prediction,y_test)\n",
    "\n",
    "\n",
    "print('{}% of samples were correctly classified'.format(str(score * 100)))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing SVM from scratch (53%)\n",
    "'''\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "# train classifier\n",
    "classifier = Karim()\n",
    "classifier.fit(x_train_scaled,y_train)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
